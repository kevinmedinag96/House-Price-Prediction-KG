# House-Price-Prediction-KG
This repository showcases my solution to the House Price Prediction problem introduced by Kaggle in its competition, reaching the best Root Mean Squared Logarithmic Error (RMSLE) of 0.1231 (top 9%). As the title implies, the task was to train an ML model to estimate housing sales prices based on the House Pricing Dataset provided by Kaggle. I have developed three notebooks obeying the main steps to develop a Data Science project, structured as follows:

1. **Notebook One - Data Wrangling**: This first notebook consists of a methodical procedure to end up with a ready-to-use numerical DataFrame. It started with a simple Exploratory Data Analysis (EDA) highlighting the variable types, which variables possess missing values & their quantity (most features with NaN values are categorical, with a few numerical ones). The next step was to handle these NaN values, strategy depended on certain factors: whether NaN values meant a default value or category, the feature's percentage of missing data and their correlation with the Sales Prices output, and data imputation for numerical features (after careful consideration, it was decided to impute these NaN values with the mean of each distribution without considering Outliers). Lastly, the categorical features were numerically encoded based on their ordinality and nominality.
2. **Notebook Two - Feature Engineering & Selection**: The first phase was to train baseline models leveraging polynomial regression, GradientBoosting, RandomForest, AdaBoost, XGB, and LightGBM. The next phase was to introduce feature selection and feature engineering to answer the question of whether selecting a subset of features and creating new ones does in fact boost the model's performance. These models were benchmarked and the results showed slight improvements toward the best baseline model (XGB and LightGBM) using Shapley values to extract the most important features, however, the best results were applying feature engineering to a subset of features and omitting the Shapley values procedure.
3. **Notebook Three - Blending models & Submission**: This final notebook trains an aggregated model (blending model) based on trained regressor models on the training dataset, i.e. a set of regression algorithms: Ridge, Lasso, Support Vector Machines, Gradient Boosting, lightGBM, XGBoost, and Stacking were trained and leveraged to derive a weighted prediction. Blended model prediction showed a boost in performance when evaluated in the Kaggle test set. The last section introduces the data processing methodology, prediction steps, and submitting the results for the unseen test set.

Based on my experimentation, the best single ML algorithms for this task follow the boosting paradigm, such as XGBoost and LightGBM.

## For running the notebooks, I suggest creating a virtual environment using Python Poetry
Install the pyproject.toml to create a virtual environment with the specified dependencies. Due to an unexpected bug, you must install the shap library manually, in order for the second notebook to work properly.

## Acknowledgements
Special thanks to Alex Lekov for their relevant contributions to designing the blending model approach with their relevant hyperparameters, as well as their feature engineering principles. See [this notebook](https://www.kaggle.com/code/itslek/blend-stack-lr-gb-0-10649-house-prices-v57/script) for further information.
